---
title: "Employee Churn Analysis"
author: "Samuel Oyedun"
format: pdf
editor: visual
---

##First Look at The Data- The Structure

Lets load in the data.

```{r}
library(plyr)
library(dplyr)
library(ggplot2)#we import ggplot
library(plotly)
library(rattle)
library(caret)
```

#Check Working Directory

```{r}
getwd()
```

```{r}
# Load the damn csv file into R like this
data =
  read.csv("MFG10YearTerminationData.csv")

head(data) #let duplicate the data into a new name


```

```{r}
str(data) #checking datatypes
```

## Second Look at The Data- Data Quality

```{r}
summary(data) #brief statistics of the data sha
```

A cursory look at the above summary doesnt have anything jump out as being data quality issues.

## Third Look at the Data - Generally What Is The Data Telling Us?

Earlier we had indicated that we had both active records at end of year and those whose status says **terminated** during the year for each of 10 years going from 2006 to 2015. To have a population to model from (to differentiate ACTIVES from TERMINATES) we have to include both status types .

Its useful then to get a baseline of what percent/proportion the terminates are of the entire population. It also answers our first question. Let's look at that next.

what proportion of staff are leaving?\_\_

```{r}
StatusCount<- as.data.frame.matrix(data %>%
group_by(STATUS_YEAR) %>% arrange(desc(STATUS_YEAR)) %>% select(STATUS) %>%table())
StatusCount
```

```{r}
StatusCount$TOTAL<-StatusCount$ACTIVE + StatusCount$TERMINATED
StatusCount$PercentTerminated = StatusCount$TERMINATED/(StatusCount$TOTAL)*100

StatusCount

mean(StatusCount$PercentTerminated)
```

It looks like it ranges from 1.97 to 4.85% with an average of 2.99%

**Where are the terminations occurring?**

Lets look at some charts

**By Business Unit**

```{r}

interactive = ggplot() + geom_bar(aes(y = ..count..,x =as.factor(BUSINESS_UNIT),fill = as.factor(STATUS)),data=data,position = position_dodge())


ggplotly(interactive, tooltip="y")
```

It looks like job termination for the last 10 years have predominantly occurred in the STORES business unit. Only 1 terminate in HR Technology which is in the head office.

Oya Lets continue to explore just the terminates for a few moments.

**Job Termination Findings According to Termination Type And Status Year**

```{r}
#just terminates
TERMINATED_Jobs = as.data.frame(data %>% filter(STATUS=="TERMINATED"))

TERMINATED_Jobs
unique(TERMINATED_Jobs$STATUS)
```

```{r}
p = as.data.frame(data %>%
  filter(STATUS=="TERMINATED"))# == means filter

p = ggplot() + geom_bar(aes(y = ..count..,x =as.factor(STATUS_YEAR),fill = as.factor(termtype_desc)),data=TERMINATED_Jobs,position = position_stack()) 


ggplotly(p, tooltip= 'y')
```

**Explanation** Generally most terminations seem to be voluntary year by year,except in the most recent years where is are some involuntary termination.

```{r}
TERMINATED_Jobs
```

**Employees that left according to Status Year and Termination Reason**

```{r}

p = ggplot() + geom_bar(aes(y = ..count.., x =as.factor(STATUS_YEAR),fill = as.factor(termreason_desc)),data=TERMINATED_Jobs, position = position_stack())

ggplotly(p, tooltip="y")


```

It seems that there were layoffs in 2014 and 2015 which accounts for the involuntary terminates.

**looking at job Termination report By Termination Reason and Department**

```{r}

ggplot() + geom_bar(aes(y = ..count..,x =as.factor(department_name),fill = as.factor(termreason_desc)),data=TERMINATED_Jobs,position = position_stack())+
  theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))

```

When we look at the terminate by Department, a few thing stick out. Customer Service has a much larger proportion of resignation compared to other departments. And retirement in general is high is a number of departments.

# Step 3 - Build Our ML Model

Remember. We have 10 years of historical data. we will use the first 9 to train the model, and the 10th year to test it. Moreover, we will use 10 fold cross validation on the training data as well. So before we actually try out a variety of modeling algorithms, we need to partition the data into training and testing datasets.

## Let's Partition The Data

```{r}
library(rattle) ### Rattle: A free graphical interface for data mining with R.  
library(magrittr) # For the %>% and %<>% operators.
library(caret)
library(lattice)
```

#It should be mentioned again that for building models, we never want to use all our data to build the model. This can lead to overfitting, where it might be able to predict well on current data that it sees as is built on, but may not predict well on data that it hasn’t seen.

```{r}
crv$seed <- 42
```

```{r}
# In Machine Learning, A pre-defined value is used to reset the random seed so that results are repeatable.

crv$seed <- 42 #setting my seed as 42
crv$seed
```

```{r}
no_of_observations = nrow(data) # 52692 observations 
no_of_observations
```

\`\`\``Time to split the data into 2 parts, one is train.csv and test.csv`

\~train.csv will be used as training model

\~test.csv will be use to test how our model is able to classify who is leaving or staying

```{r}
training_2006_2014 = MYtrain = subset(data,STATUS_YEAR<=2014)#2006:2014


for_testing_2015 = subset(data,STATUS_YEAR== 2015)#filter only 2015

head(training_2006_2014)
nrow(training_2006_2014)
nrow(for_testing_2015)

#now export them to csv
#write.csv(for_testing_2015,'test data.csv')
#write.csv(training_2006_2014,'train data.csv')
```

#now lets start doing Train-Test Split #Meaning we are going to split the data

#Called this library

```{r}

#Create training and testing datasets


set.seed(crv$seed)#setting the seed so everyone can have same fucking result o 
MYnobs <- nrow(data) # 49653 observations

#lETS FILter our only 2014 to train
MYsample = MYtrain = subset(data,STATUS_YEAR<=2014)


MYvalidate = NULL
MYtest = subset(data,STATUS_YEAR== 2015)#2015 will be used for testing

# These are my  variable selections  I will be using as predictors

MYinput = c("age", "length_of_service","gender_full",
               "STATUS_YEAR", "BUSINESS_UNIT") #these are my input columns combination

MYnumeric = c("age", "length_of_service", "STATUS_YEAR")

MYcategoric = c("gender_full", "BUSINESS_UNIT")

MYtarget  = "STATUS" #na this column i want predict 
MYrisk    = NULL 
MYident   = "EmployeeID" #this is my identity column

MYignore  = c("recorddate_key", "birthdate_key", "orighiredate_key", "terminationdate_key", "city_name", "gender_short", "termreason_desc", "termtype_desc","department_name",
"job_title", "store_name")

MYweights = NULL

MYTrainingData = MYtrain[c(MYinput, MYtarget)]
MYTestingData = MYtest[c(MYinput, MYtarget)]

```

```{r}
nrow(MYTrainingData)
nrow(MYTestingData)
```

```{r}
MYTrainingData
```

## Choosing and Running Models

One of the things that characterizes R, is that the number of functions and procedures that can be used are huge. So there often many ways of doing things. Two of the best R packages designed to be used for data science I sort of like are **caret** and **rattle**

What is noteworthy about rattle is that it provides a GUI front end and generates the code for it in the log on the backend. So you can generate models quickly.

We should step back for a moment and review what we doing here, and what are opening questions were. We are hoping to predict who might leave is or her job in the future. That is a 'binary result' or 'category'. A person is either 'ACTIVE' or 'TERMINATED'. Since it is a category to be predicted we will choose among models/algorithms that can predict categories.

```         
  The models we will look at in rattle are:
```

-   Decision Trees (rpart)
-   Boosted Models (adaboost)
-   Random Forests (rf)
-   Support Vector Models (svm)
-   Linear Models (glm)

###Decision Tree

Lets first u take a look at a decision tree model. This is always useful because with these, you can get a visual tree model to get some idea of how the prediction occurs in an easy to understand way.

```{r}
library(rpart, quietly=TRUE)

# Reset the random number seed to obtain the same results each time.

set.seed(crv$seed)

# Build the Decision Tree model.

MYrpart = rpart(STATUS ~ .,
                   data = MYtrain[ c(MYinput, MYtarget)],
                   method="class",
                   parms=list(split="information"))



print(MYrpart)




fancyRpartPlot(MYrpart, main="Decision Tree For Employee Churn $ STATUS")
```

-   Root Node: Represents the entire sample population and this is futher divided into two or more homogeneous groups
-   Split : The criterion used to divide a node into two or more sub-nodes
-   n : The number of observations in a node
-   Loss : This is the total number of rows that will be misclassified if the predicted class for the node is applied to all rows
-   yval: The Overall prediction for the branch( Yes or No). In general, this is the mean response value for that subset
-   yprob: The fraction of observations in that branch that take on values of Yes and No.

```{r}
rattle()
```

**What, if anything, else contributes to it?**

`From even the graphical tree output it looks like gender, and status year also affect employee decision to leave in the future.`

\*IN FACT, Root node error is the percent of correctly sorted records at the first (root) splitting node.

\`Pruning is a data compression technique in machine learning and that reduces the size of decision trees by removing sections of the tree that are non-critical and redundant to classify instances. Pruning reduces the complexity of the final classifier, and hence improves predictive accuracy by the reduction of overfitting.

That summary table info you saw there upstairs is telling us some few shits like that Rel error (relative error) is 1 – R2 root mean square error. This is the error for predictions of the data that were used to estimate the model. The x-error is the cross-validation error (generated by the rpart built-in cross validation). Each level in the Pruning table is the depth of the tree where each of the corresponding values were calculated. This can help you make a decision on where to prune the tree.

```{r}
MYpr =  predict(MYrpart, newdata=MYtest[c(MYinput, MYtarget)], type="class")

MYpr

# Generate the confusion matrix showing counts.


```

```{r}
table(MYtest[c(MYinput, MYtarget)]$STATUS, MYpr,
      dnn=c("Actual", "Predicted"))
```

```{r}
rattle()
```

```{r}

library(randomForest, quietly=TRUE)

# Build the Random Forest model.

set.seed(crv$seed)
MYrf <- randomForest::randomForest(STATUS ~ .,data=MYtrain[c(MYinput, MYtarget)],ntree=500,mtry=2, importance=TRUE,                        na.action=randomForest::na.roughfix,replace=FALSE)

# Generate textual output of 'Random Forest' model.

MYrf

# The `pROC' package implements various AUC functions.

# Calculate the Area Under the Curve (AUC).

pROC::roc(MYrf$y, as.numeric(MYrf$predicted))

# Calculate the AUC Confidence Interval.

pROC::ci.auc(MYrf$y, as.numeric(MYrf$predicted))

# List the importance of the variables.

rn <- round(randomForest::importance(MYrf), 2)
rn[order(rn[,3], decreasing=TRUE),]

```

### Adaboost

Now for adaboost

```{r}

set.seed(crv$seed)
MYada <- ada::ada(STATUS ~ ., data=MYtrain[c(MYinput, MYtarget)],control=rpart::rpart.control(maxdepth=30,cp=0.010000,minsplit=20,xval=10),iter=50)

# Print the results of the modelling.

print(MYada)
round(MYada$model$errs[MYada$iter,], 2)
cat('Variables actually used in tree construction:\n')
print(sort(names(listAdaVarsUsed(MYada))))
cat('\nFrequency of variables actually used:\n')
print(listAdaVarsUsed(MYada))


```

###Support Vector Machines

Now lets look at Support Vector Machines

```{r}

#============================================================
# Rattle timestamp: 2016-03-25 18:22:56 x86_64-w64-mingw32 

# Support vector machine. 

# The 'kernlab' package provides the 'ksvm' function.

library(kernlab, quietly=TRUE)

# Build a Support Vector Machine model.

set.seed(crv$seed)
MYksvm <- ksvm(as.factor(STATUS) ~ .,
               data=MYtrain[c(MYinput, MYtarget)],
               kernel="rbfdot",
               prob.model=TRUE)

# Generate a textual view of the SVM model.

MYksvm

# Time taken: 42.91 secs


```

###Linear Models

Finally lets look at linear models.

```{r}
#============================================================
# Rattle timestamp: 2016-03-25 18:23:56 x86_64-w64-mingw32 

# Regression model 

# Build a Regression model.

MYglm <- glm(STATUS ~ .,
             data=MYtrain[c(MYinput, MYtarget)],
             family=binomial(link="logit"))

# Generate a textual view of the Linear model.

print(summary(MYglm))
cat(sprintf("Log likelihood: %.3f (%d df)\n",
            logLik(MYglm)[1],
            attr(logLik(MYglm), "df")))
cat(sprintf("Null/Residual deviance difference: %.3f (%d df)\n",
            MYglm$null.deviance-MYglm$deviance,
            MYglm$df.null-MYglm$df.residual))
cat(sprintf("Chi-square p-value: %.8f\n",
            dchisq(MYglm$null.deviance-MYglm$deviance,
                   MYglm$df.null-MYglm$df.residual)))
cat(sprintf("Pseudo R-Square (optimistic): %.8f\n",
            cor(MYglm$y, MYglm$fitted.values)))
cat('\n==== ANOVA ====\n\n')
print(anova(MYglm, test="Chisq"))
cat("\n")

# Time taken: 1.62 secs


```

These were simply the vanilla running of these models.In evaluating the models we have the means to compare their results on a common basis.

##Evaluate Models

In the evaluating models step, we are able to answer our final 2 original questions stated at the beginning:

**Can we predict?**

In a word 'yes'.

**How Well can we predict?**

In two words 'fairly well'.

When it comes to evaluating models for predicting categories, we are defining accuracy as to how many times did the model predict the actual. So we are interested in a number of things.

The first of these are error martricies. In error matricies, you are cross tabulating the actual results with predicted results. If prediction was 'perfect' 100%, every prediction would be the same as actual. (almost never happens). The higher the correct prediction rate and lower the error rate- the better.

###Error Matricies

####Decision Trees

```{r}

#============================================================
# Rattle timestamp: 2016-03-25 18:50:22 x86_64-w64-mingw32 

# Evaluate model performance. 

# Generate an Error Matrix for the Decision Tree model.

# Obtain the response from the Decision Tree model.

MYpr <- predict(MYrpart, newdata=MYtest[c(MYinput, MYtarget)], type="class")

# Generate the confusion matrix showing counts.

table(MYtest[c(MYinput, MYtarget)]$STATUS, MYpr,
      dnn=c("Actual", "Predicted"))

# Generate the confusion matrix showing proportions.

pcme <- function(actual, cl)
{
  x <- table(actual, cl)
  nc <- nrow(x)
  tbl <- cbind(x/length(actual),
               Error=sapply(1:nc,
                            function(r) round(sum(x[r,-r])/sum(x[r,]), 2)))
  names(attr(tbl, "dimnames")) <- c("Actual", "Predicted")
  return(tbl)
}
per <- pcme(MYtest[c(MYinput, MYtarget)]$STATUS, MYpr)
round(per, 2)

# Calculate the overall error percentage.

cat(100*round(1-sum(diag(per), na.rm=TRUE), 2))

# Calculate the averaged class error percentage.

cat(100*round(mean(per[,"Error"], na.rm=TRUE), 2))
```

####Adaboost

```{r}

# Generate an Error Matrix for the Ada Boost model.

# Obtain the response from the Ada Boost model.

MYpr <- predict(MYada, newdata=MYtest[c(MYinput, MYtarget)])

# Generate the confusion matrix showing counts.

table(MYtest[c(MYinput, MYtarget)]$STATUS, MYpr,
      dnn=c("Actual", "Predicted"))

# Generate the confusion matrix showing proportions.

pcme <- function(actual, cl)
{
  x <- table(actual, cl)
  nc <- nrow(x)
  tbl <- cbind(x/length(actual),
               Error=sapply(1:nc,
                            function(r) round(sum(x[r,-r])/sum(x[r,]), 2)))
  names(attr(tbl, "dimnames")) <- c("Actual", "Predicted")
  return(tbl)
}
per <- pcme(MYtest[c(MYinput, MYtarget)]$STATUS, MYpr)
round(per, 2)

# Calculate the overall error percentage.

cat(100*round(1-sum(diag(per), na.rm=TRUE), 2))

# Calculate the averaged class error percentage.

cat(100*round(mean(per[,"Error"], na.rm=TRUE), 2))

```

####Random Forest

```{r}

# Generate an Error Matrix for the Random Forest model.

# Obtain the response from the Random Forest model.

MYpr <- predict(MYrf, newdata=na.omit(MYtest[c(MYinput, MYtarget)]))

# Generate the confusion matrix showing counts.

table(na.omit(MYtest[c(MYinput, MYtarget)])$STATUS, MYpr,
      dnn=c("Actual", "Predicted"))

# Generate the confusion matrix showing proportions.

pcme <- function(actual, cl)
{
  x <- table(actual, cl)
  nc <- nrow(x)
  tbl <- cbind(x/length(actual),
               Error=sapply(1:nc,
                            function(r) round(sum(x[r,-r])/sum(x[r,]), 2)))
  names(attr(tbl, "dimnames")) <- c("Actual", "Predicted")
  return(tbl)
}
per <- pcme(na.omit(MYtest[c(MYinput, MYtarget)])$STATUS, MYpr)
round(per, 2)

# Calculate the overall error percentage.

cat(100*round(1-sum(diag(per), na.rm=TRUE), 2))

# Calculate the averaged class error percentage.

cat(100*round(mean(per[,"Error"], na.rm=TRUE), 2))

```

####Support Vector Model

```{r}

# Generate an Error Matrix for the SVM model.

# Obtain the response from the SVM model.

MYpr <- kernlab::predict(MYksvm, newdata=na.omit(MYtest[c(MYinput, MYtarget)]))

# Generate the confusion matrix showing counts.

table(na.omit(MYtest[c(MYinput, MYtarget)])$STATUS, MYpr,
      dnn=c("Actual", "Predicted"))

# Generate the confusion matrix showing proportions.

pcme <- function(actual, cl)
{
  x <- table(actual, cl)
  nc <- nrow(x)
  tbl <- cbind(x/length(actual),
               Error=sapply(1:nc,
                            function(r) round(sum(x[r,-r])/sum(x[r,]), 2)))
  names(attr(tbl, "dimnames")) <- c("Actual", "Predicted")
  return(tbl)
}
per <- pcme(na.omit(MYtest[c(MYinput, MYtarget)])$STATUS, MYpr)
round(per, 2)

# Calculate the overall error percentage.

cat(100*round(1-sum(diag(per), na.rm=TRUE), 2))

# Calculate the averaged class error percentage.

cat(100*round(mean(per[,"Error"], na.rm=TRUE), 2))
```

####Linear Model

```{r}

# Generate an Error Matrix for the Linear model.

# Obtain the response from the Linear model.

MYpr <- as.vector(ifelse(predict(MYglm, type="response", newdata=MYtest[c(MYinput, MYtarget)]) > 0.5, "TERMINATED", "ACTIVE"))

# Generate the confusion matrix showing counts.

table(MYtest[c(MYinput, MYtarget)]$STATUS, MYpr,
      dnn=c("Actual", "Predicted"))

# Generate the confusion matrix showing proportions.

pcme <- function(actual, cl)
{
  x <- table(actual, cl)
  nc <- nrow(x)
  tbl <- cbind(x/length(actual),
               Error=sapply(1:nc,
                            function(r) round(sum(x[r,-r])/sum(x[r,]), 2)))
  names(attr(tbl, "dimnames")) <- c("Actual", "Predicted")
  return(tbl)
}
per <- pcme(MYtest[c(MYinput, MYtarget)]$STATUS, MYpr)
round(per, 2)

# Calculate the overall error percentage.

cat(100*round(1-sum(diag(per), na.rm=TRUE), 2))

# Calculate the averaged class error percentage.

cat(100*round(mean(per[,"Error"], na.rm=TRUE), 2))


```

**Well that was interesting!**

Summarizing the confusion matrix showed that decision trees,random forests, and adaboost all predicted similarly. **BUT** Support Vector Machines and the Linear Models did worse for this data.

###Area Under Curve (AUC)

Another way to evaluate the models is the AUC. The higher the AUC the better. The code below generates the information necessary to produce the graphs.

```{r}

#============================================================
# Rattle timestamp: 2016-03-25 19:44:22 x86_64-w64-mingw32 

# Evaluate model performance. 

# ROC Curve: requires the ROCR package.

library(ROCR)

# ROC Curve: requires the ggplot2 package.

library(ggplot2, quietly=TRUE)

# Generate an ROC Curve for the rpart model on MFG10YearTerminationData [test].

MYpr <- predict(MYrpart, newdata=MYtest[c(MYinput, MYtarget)])[,2]

# Remove observations with missing target.

no.miss   <- na.omit(MYtest[c(MYinput, MYtarget)]$STATUS)
miss.list <- attr(no.miss, "na.action")
attributes(no.miss) <- NULL

if (length(miss.list))
{
  pred <- prediction(MYpr[-miss.list], no.miss)
} else
{
  pred <- prediction(MYpr, no.miss)
}

pe <- performance(pred, "tpr", "fpr")
au <- performance(pred, "auc")@y.values[[1]]
pd <- data.frame(fpr=unlist(pe@x.values), tpr=unlist(pe@y.values))
p <- ggplot(pd, aes(x=fpr, y=tpr))
p <- p + geom_line(colour="red")
p <- p + xlab("False Positive Rate") + ylab("True Positive Rate")
p <- p + ggtitle("ROC Curve Decision Tree MFG10YearTerminationData [test] STATUS")
p <- p + theme(plot.title=element_text(size=10))
p <- p + geom_line(data=data.frame(), aes(x=c(0,1), y=c(0,1)), colour="grey")
p <- p + annotate("text", x=0.50, y=0.00, hjust=0, vjust=0, size=5,
                  label=paste("AUC =", round(au, 2)))
print(p)

# Calculate the area under the curve for the plot.


# Remove observations with missing target.

no.miss   <- na.omit(MYtest[c(MYinput, MYtarget)]$STATUS)
miss.list <- attr(no.miss, "na.action")
attributes(no.miss) <- NULL

if (length(miss.list))
{
  pred <- prediction(MYpr[-miss.list], no.miss)
} else
{
  pred <- prediction(MYpr, no.miss)
}
performance(pred, "auc")

# ROC Curve: requires the ROCR package.

library(ROCR)

# ROC Curve: requires the ggplot2 package.

library(ggplot2, quietly=TRUE)

# Generate an ROC Curve for the ada model on MFG10YearTerminationData [test].

MYpr <- predict(MYada, newdata=MYtest[c(MYinput, MYtarget)], type="prob")[,2]

# Remove observations with missing target.

no.miss   <- na.omit(MYtest[c(MYinput, MYtarget)]$STATUS)
miss.list <- attr(no.miss, "na.action")
attributes(no.miss) <- NULL

if (length(miss.list))
{
  pred <- prediction(MYpr[-miss.list], no.miss)
} else
{
  pred <- prediction(MYpr, no.miss)
}

pe <- performance(pred, "tpr", "fpr")
au <- performance(pred, "auc")@y.values[[1]]
pd <- data.frame(fpr=unlist(pe@x.values), tpr=unlist(pe@y.values))
p <- ggplot(pd, aes(x=fpr, y=tpr))
p <- p + geom_line(colour="red")
p <- p + xlab("False Positive Rate") + ylab("True Positive Rate")
p <- p + ggtitle("ROC Curve Ada Boost MFG10YearTerminationData [test] STATUS")
p <- p + theme(plot.title=element_text(size=10))
p <- p + geom_line(data=data.frame(), aes(x=c(0,1), y=c(0,1)), colour="grey")
p <- p + annotate("text", x=0.50, y=0.00, hjust=0, vjust=0, size=5,
                  label=paste("AUC =", round(au, 2)))
print(p)

# Calculate the area under the curve for the plot.


# Remove observations with missing target.

no.miss   <- na.omit(MYtest[c(MYinput, MYtarget)]$STATUS)
miss.list <- attr(no.miss, "na.action")
attributes(no.miss) <- NULL

if (length(miss.list))
{
  pred <- prediction(MYpr[-miss.list], no.miss)
} else
{
  pred <- prediction(MYpr, no.miss)
}
performance(pred, "auc")

# ROC Curve: requires the ROCR package.

library(ROCR)

# ROC Curve: requires the ggplot2 package.

library(ggplot2, quietly=TRUE)

# Generate an ROC Curve for the rf model on MFG10YearTerminationData [test].

MYpr <- predict(MYrf, newdata=na.omit(MYtest[c(MYinput, MYtarget)]), type="prob")[,2]

# Remove observations with missing target.

no.miss   <- na.omit(na.omit(MYtest[c(MYinput, MYtarget)])$STATUS)
miss.list <- attr(no.miss, "na.action")
attributes(no.miss) <- NULL

if (length(miss.list))
{
  pred <- prediction(MYpr[-miss.list], no.miss)
} else
{
  pred <- prediction(MYpr, no.miss)
}

pe <- performance(pred, "tpr", "fpr")
au <- performance(pred, "auc")@y.values[[1]]
pd <- data.frame(fpr=unlist(pe@x.values), tpr=unlist(pe@y.values))
p <- ggplot(pd, aes(x=fpr, y=tpr))
p <- p + geom_line(colour="red")
p <- p + xlab("False Positive Rate") + ylab("True Positive Rate")
p <- p + ggtitle("ROC Curve Random Forest MFG10YearTerminationData [test] STATUS")
p <- p + theme(plot.title=element_text(size=10))
p <- p + geom_line(data=data.frame(), aes(x=c(0,1), y=c(0,1)), colour="grey")
p <- p + annotate("text", x=0.50, y=0.00, hjust=0, vjust=0, size=5,
                  label=paste("AUC =", round(au, 2)))
print(p)

# Calculate the area under the curve for the plot.


# Remove observations with missing target.

no.miss   <- na.omit(na.omit(MYtest[c(MYinput, MYtarget)])$STATUS)
miss.list <- attr(no.miss, "na.action")
attributes(no.miss) <- NULL

if (length(miss.list))
{
  pred <- prediction(MYpr[-miss.list], no.miss)
} else
{
  pred <- prediction(MYpr, no.miss)
}
performance(pred, "auc")

# ROC Curve: requires the ROCR package.

library(ROCR)

# ROC Curve: requires the ggplot2 package.

library(ggplot2, quietly=TRUE)

# Generate an ROC Curve for the ksvm model on MFG10YearTerminationData [test].

MYpr <- kernlab::predict(MYksvm, newdata=na.omit(MYtest[c(MYinput, MYtarget)]), type="probabilities")[,2]

# Remove observations with missing target.

no.miss   <- na.omit(na.omit(MYtest[c(MYinput, MYtarget)])$STATUS)
miss.list <- attr(no.miss, "na.action")
attributes(no.miss) <- NULL

if (length(miss.list))
{
  pred <- prediction(MYpr[-miss.list], no.miss)
} else
{
  pred <- prediction(MYpr, no.miss)
}

pe <- performance(pred, "tpr", "fpr")
au <- performance(pred, "auc")@y.values[[1]]
pd <- data.frame(fpr=unlist(pe@x.values), tpr=unlist(pe@y.values))
p <- ggplot(pd, aes(x=fpr, y=tpr))
p <- p + geom_line(colour="red")
p <- p + xlab("False Positive Rate") + ylab("True Positive Rate")
p <- p + ggtitle("ROC Curve SVM MFG10YearTerminationData [test] STATUS")
p <- p + theme(plot.title=element_text(size=10))
p <- p + geom_line(data=data.frame(), aes(x=c(0,1), y=c(0,1)), colour="grey")
p <- p + annotate("text", x=0.50, y=0.00, hjust=0, vjust=0, size=5,
                  label=paste("AUC =", round(au, 2)))
print(p)

# Calculate the area under the curve for the plot.


# Remove observations with missing target.

no.miss   <- na.omit(na.omit(MYtest[c(MYinput, MYtarget)])$STATUS)
miss.list <- attr(no.miss, "na.action")
attributes(no.miss) <- NULL

if (length(miss.list))
{
  pred <- prediction(MYpr[-miss.list], no.miss)
} else
{
  pred <- prediction(MYpr, no.miss)
}
performance(pred, "auc")

# ROC Curve: requires the ROCR package.

library(ROCR)

# ROC Curve: requires the ggplot2 package.

library(ggplot2, quietly=TRUE)

# Generate an ROC Curve for the glm model on MFG10YearTerminationData [test].

MYpr <- predict(MYglm, type="response", newdata=MYtest[c(MYinput, MYtarget)])

# Remove observations with missing target.

no.miss   <- na.omit(MYtest[c(MYinput, MYtarget)]$STATUS)
miss.list <- attr(no.miss, "na.action")
attributes(no.miss) <- NULL

if (length(miss.list))
{
  pred <- prediction(MYpr[-miss.list], no.miss)
} else
{
  pred <- prediction(MYpr, no.miss)
}

pe <- performance(pred, "tpr", "fpr")
au <- performance(pred, "auc")@y.values[[1]]
pd <- data.frame(fpr=unlist(pe@x.values), tpr=unlist(pe@y.values))
p <- ggplot(pd, aes(x=fpr, y=tpr))
p <- p + geom_line(colour="red")
p <- p + xlab("False Positive Rate") + ylab("True Positive Rate")
p <- p + ggtitle("ROC Curve Linear MFG10YearTerminationData [test] STATUS")
p <- p + theme(plot.title=element_text(size=10))
p <- p + geom_line(data=data.frame(), aes(x=c(0,1), y=c(0,1)), colour="grey")
p <- p + annotate("text", x=0.50, y=0.00, hjust=0, vjust=0, size=5,
                  label=paste("AUC =", round(au, 2)))
print(p)

# Calculate the area under the curve for the plot.


# Remove observations with missing target.

no.miss   <- na.omit(MYtest[c(MYinput, MYtarget)]$STATUS)
miss.list <- attr(no.miss, "na.action")
attributes(no.miss) <- NULL

if (length(miss.list))
{
  pred <- prediction(MYpr[-miss.list], no.miss)
} else
{
  pred <- prediction(MYpr, no.miss)
}
performance(pred, "auc")




```

A couple of things to notice:

-   **It turns out that the adaboost model produces the highest AUC.** So we will use it to predict the 2016 terminates in just a little bit.
-   The Linear model was worst.

#Deploy Model

Lets predict the 2016 Terminates.

In real life you would take a snapshot of data at end of 2015 fof active employees. For purposes of this exercise we will do that but also alter the data to make both age year of service information - 1 year greater for 2016.

```{r}
#Apply model
#Generate 2016 data
Employees2016<-MYtest #2015 data

ActiveEmployees2016<-subset(Employees2016,STATUS=='ACTIVE')
ActiveEmployees2016$age<-ActiveEmployees2016$age+1
ActiveEmployees2016$length_of_service<-ActiveEmployees2016$length_of_service+1

#Predict 2016 Terminates using adaboost
#MYada was name we gave to adaboost model earlier
ActiveEmployees2016$PredictedSTATUS2016<-predict(MYada,ActiveEmployees2016)
PredictedTerminatedEmployees2016<-subset(ActiveEmployees2016,PredictedSTATUS2016=='TERMINATED')
#show records for first 5 predictions
head(PredictedTerminatedEmployees2016)
```

There were 93 predicted terminates for 2016.

#Wrap Up

The intent of this project was:

-   to once again demonstrate a People Analytics example Using R
-   to demonstrate a meaningful example from the HR context
-   not to be necessarily a best practices example, rather an illustrative one
-   to motivate the HR Community to make much more extensive use of 'data driven' HR decision making.
-   to encourage those interested in using R in data science to delve more deeply into R's tools in this area.

From a practical perpsective, if this was real data from a real organization, the onus would be on the organization to make 'decisions' about what the data is telling them.
